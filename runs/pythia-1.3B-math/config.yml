# For fine-tuning on my (Yale LILY) lab server
experiment_name: pythia-1.3B-math
task: "math"
model_load_path: "EleutherAI/pythia-1.3b"
log_dir: "runs"

max_length: 2048

lr: 2.e-5
lr_schedule: "cosine"
warmup_steps: 200
train_steps: 4000
log_steps: 50
save_steps: 2000
batch_size_per_device: 1 # want effective batch size of 32 across experiments
gradient_accumulation: 32
weight_decay: 0.01

