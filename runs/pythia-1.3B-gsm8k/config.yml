# For fine-tuning on my (Yale LILY) lab server
experiment_name: pythia-1.3B-gsm8k
task: "gsm8k"
model_load_path: "EleutherAI/pythia-1.3b"
log_dir: "runs"

max_length: 500

lr: 2.e-5
lr_schedule: "cosine"
warmup_steps: 200
train_steps: 4000
log_steps: 50
save_steps: 2000
batch_size_per_device: 8 # want effective batch size of 32 across experiments
gradient_accumulation: 4
weight_decay: 0.01

